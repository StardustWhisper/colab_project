{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StardustWhisper/colab_project/blob/main/cagliostro_forge_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![visitor][visitor-badge]][visitor-stats]\n",
        "\n",
        "# **Cagliostro Forge Colab**\n",
        "Rise from the ashes, reborn and empowered by [lllyasviel/stable-diffusion-webui-forge](https://github.com/lllyasviel/stable-diffusion-webui-forge)\n",
        "\n",
        "**Version 1.0.0** | [Github][link-to-github] | [License](https://github.com/cagliostrolab/forge-colab/blob/main/LICENSE)\n",
        "\n",
        "[visitor-badge]: https://api.visitorbadge.io/api/visitors?path=cagliostro-forge-colab&label=Visitors&labelColor=%2334495E&countColor=%231ABC9C&style=flat&labelStyle=none\n",
        "[visitor-stats]: https://visitorbadge.io/status?path=cagliostro-forge-colab\n",
        "[link-to-github]: https://github.com/cagliostrolab/forge-colab/blob/main/cagliostro-forge-colab.ipynb"
      ],
      "metadata": {
        "id": "GnmYPpV3o719"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## **Install Environment**\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import string\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from pydantic import BaseModel\n",
        "\n",
        "python_version  = \".\".join(sys.version.split(\".\")[:2])\n",
        "python_path     = Path(f\"/usr/local/lib/python{python_version}/dist-packages/\")\n",
        "colablib_path   = python_path / \"colablib\"\n",
        "if not colablib_path.exists():\n",
        "    subprocess.run(['pip', 'install', '--upgrade', 'git+https://github.com/Linaqruf/colablib'], check=True)\n",
        "\n",
        "from colablib.colored_print import cprint, print_line\n",
        "from colablib.utils import py_utils, package_utils, config_utils\n",
        "from colablib.sd_models.downloader import aria2_download, download\n",
        "from colablib.utils.git_utils import update_repo, reset_repo, validate_repo, batch_update\n",
        "from colablib.utils.py_utils import get_filename\n",
        "\n",
        "################################\n",
        "# COLAB ARGUMENTS GOES HERE\n",
        "################################\n",
        "\n",
        "# It ain't much, but it's honest work.\n",
        "class CustomDirs(BaseModel):\n",
        "    url: str\n",
        "    dst: str\n",
        "\n",
        "# @markdown ### **Drive Config**\n",
        "mount_drive          = True  # @param {type: 'boolean'}\n",
        "output_drive_folder  = \"cagliostro-colab-forge\"  # @param {type: 'string'}\n",
        "\n",
        "# @markdown ### **Repo Config**\n",
        "update_webui         = True  # @param {type: 'boolean'}\n",
        "update_extensions    = True  # @param {type: 'boolean'}\n",
        "commit_hash          = \"\"  # @param {type: 'string'}\n",
        "\n",
        "# @markdown ### **Download Config**\n",
        "# @markdown > Check only the options you need\n",
        "animagine_xl_3_1     = True  # @param {type: 'boolean'}\n",
        "rae_diffusion_xl_v2  = False  # @param {type: 'boolean'}\n",
        "kivotos_xl_v2_0      = False  # @param {type: 'boolean'}\n",
        "urangdiffusion_1_4   = False  # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown > **Note:**\n",
        "# @markdown - For multiple URLs, use comma separation (e.g. `url1, url2, url3`)\n",
        "# @markdown - Forge supports FLUX, SD, and SDXL, but this notebook focuses only on SDXL\n",
        "# @markdown - **Highly Recommended:** Use Hugging Face links whenever possible\n",
        "custom_model_url     = \"\"  # @param {'type': 'string'}\n",
        "custom_vae_url       = \"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl.vae.safetensors\"  # @param {'type': 'string'}\n",
        "custom_lora_url      = \"\"  # @param {'type': 'string'}\n",
        "\n",
        "# @markdown ### **Tunnel Config**\n",
        "# @markdown > Default to `--share` until `ngrok_token` is not `None`\n",
        "ngrok_token          = \"\"  # @param {type: 'string'}\n",
        "ngrok_region         = \"ap\"  # @param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n",
        "\n",
        "# @markdown ### **UI/UX Config**\n",
        "gradio_theme         = \"remilia/Ghostly\"  # @param [\"Default\", \"gradio/base\", \"gradio/glass\", \"gradio/monochrome\", \"gradio/seafoam\", \"gradio/soft\", \"gradio/dracula_test\", \"abidlabs/dracula_test\", \"abidlabs/Lime\", \"abidlabs/pakistan\", \"Ama434/neutral-barlow\", \"dawood/microsoft_windows\", \"finlaymacklon/smooth_slate\", \"Franklisi/darkmode\", \"freddyaboulton/dracula_revamped\", \"freddyaboulton/test-blue\", \"gstaff/xkcd\", \"Insuz/Mocha\", \"Insuz/SimpleIndigo\", \"JohnSmith9982/small_and_pretty\", \"nota-ai/theme\", \"nuttea/Softblue\", \"ParityError/Anime\", \"reilnuud/polite\", \"remilia/Ghostly\", \"rottenlittlecreature/Moon_Goblin\", \"step-3-profit/Midnight-Deep\", \"Taithrah/Minimal\", \"ysharma/huggingface\", \"ysharma/steampunk\", \"NoCrypt/miku\"]\n",
        "# @markdown Set `use_preset` for using default prompt, resolution, sampler, and other settings\n",
        "use_presets          = True  # @param {type: 'boolean'}\n",
        "\n",
        "# @markdown ### **Launch Arguments**\n",
        "use_gradio_auth      = False  # @param {type: 'boolean'}\n",
        "auto_select_model    = False  # @param {type: 'boolean'}\n",
        "auto_select_vae      = True  # @param {type: 'boolean'}\n",
        "additional_arguments = \"--lowram --theme dark --no-half-vae --opt-sdp-attention\"  # @param {type: 'string'}\n",
        "\n",
        "################################\n",
        "# GLOBAL VARIABLES GOES HERE\n",
        "################################\n",
        "\n",
        "# GRADIO AUTH\n",
        "user      = \"cagliostro\"\n",
        "password  = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
        "\n",
        "# ROOT DIR\n",
        "root_dir        = Path(\"/content\")\n",
        "drive_dir       = root_dir / \"drive\" / \"MyDrive\"\n",
        "repo_dir        = root_dir / \"stable-diffusion-webui-forge\"\n",
        "tmp_dir         = root_dir / \"tmp\"\n",
        "\n",
        "models_dir      = repo_dir / \"models\"\n",
        "extensions_dir  = repo_dir / \"extensions\"\n",
        "ckpt_dir        = models_dir / \"Stable-diffusion\"\n",
        "vae_dir         = models_dir / \"VAE\"\n",
        "lora_dir        = models_dir / \"Lora\"\n",
        "output_subdir   = [\"txt2img-samples\", \"img2img-samples\", \"extras-samples\", \"txt2img-grids\", \"img2img-grids\"]\n",
        "\n",
        "config_file_path    = repo_dir / \"config.json\"\n",
        "ui_config_file_path = repo_dir / \"ui-config.json\"\n",
        "\n",
        "package_url = [\n",
        "    \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-forge.tar.lz4\",\n",
        "    \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-forge-deps.tar.lz4\",\n",
        "]\n",
        "\n",
        "custom_dirs = {\n",
        "    \"model\" : CustomDirs(url=custom_model_url, dst=str(ckpt_dir)),\n",
        "    \"vae\"   : CustomDirs(url=custom_vae_url, dst=str(vae_dir)),\n",
        "    \"lora\"  : CustomDirs(url=custom_lora_url, dst=str(lora_dir)),\n",
        "}\n",
        "\n",
        "default_model_urls = {\n",
        "    \"animagine_xl_3_1\"      : \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\",\n",
        "    \"rae_diffusion_xl_v2\"   : \"https://huggingface.co/Raelina/Rae-Diffusion-XL-V2/resolve/main/RaeDiffusion-XL-v2.safetensors\",\n",
        "    \"kivotos_xl_v2_0\"       : \"https://huggingface.co/yodayo-ai/kivotos-xl-2.0/resolve/main/kivotos-xl-2.0.safetensors\",\n",
        "    \"urangdiffusion_1_4\"    : \"https://huggingface.co/kayfahaarukku/UrangDiffusion-1.4/resolve/main/UrangDiffusion-1.4.safetensors\",\n",
        "}\n",
        "\n",
        "################################\n",
        "# HELPER FUNCTIONS STARTS HERE\n",
        "################################\n",
        "\n",
        "def mount_drive_function(directory):\n",
        "    output_dir = repo_dir / \"outputs\"\n",
        "\n",
        "    if mount_drive:\n",
        "        print_line(80, color=\"green\")\n",
        "        if not directory.exists():\n",
        "            from google.colab import drive\n",
        "            cprint(\"Mounting google drive...\", color=\"green\", reset=False)\n",
        "            drive.mount(str(directory.parent))\n",
        "        output_dir = directory / output_drive_folder\n",
        "        cprint(\"Set default output path to:\", output_dir, color=\"green\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def setup_directories():\n",
        "    for dir in [ckpt_dir, vae_dir, lora_dir]:\n",
        "        dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def pre_download(dir, urls, desc, overwrite=False):\n",
        "    ffmpy_path = python_path / \"ffmpy-0.3.0.dist-info\"\n",
        "\n",
        "    for url in tqdm(urls, desc=desc):\n",
        "        filename = Path(url).name\n",
        "        aria2_download(dir, filename, url, quiet=True)\n",
        "        if filename == \"webui-forge-deps.tar.lz4\":\n",
        "            package_utils.extract_package(filename, python_path, overwrite=True)\n",
        "        else:\n",
        "            package_utils.extract_package(filename, \"/\", overwrite=overwrite)\n",
        "        os.remove(dir / filename)\n",
        "\n",
        "    subprocess.run([\"rm\", \"-rf\", str(ffmpy_path)])\n",
        "    subprocess.run([\"pip\", \"install\", \"--force-reinstall\", \"ffmpy\"], check=True)\n",
        "\n",
        "def install_dependencies():\n",
        "    ubuntu_deps = [\"aria2\", \"lz4\"]\n",
        "    cprint(\"Installing ubuntu dependencies\", color=\"green\")\n",
        "    subprocess.run([\"apt\", \"install\", \"-y\"] + ubuntu_deps, check=True)\n",
        "\n",
        "def install_webui(repo_dir, desc):\n",
        "    if not repo_dir.exists():\n",
        "        pre_download(root_dir, package_url, desc, overwrite=False)\n",
        "    else:\n",
        "        cprint(\"Stable Diffusion Web UI Forge already installed, skipping...\", color=\"green\")\n",
        "\n",
        "def configure_output_path(config_path, output_dir, output_subdir):\n",
        "    try:\n",
        "        config = config_utils.read_config(str(config_path))\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        config = {}\n",
        "\n",
        "    config_updates = {\n",
        "        f\"outdir_{subdir.split('-')[0]}_{'_'.join(subdir.split('-')[1:])}\": str(output_dir / subdir)\n",
        "        for subdir in output_subdir\n",
        "    }\n",
        "    config.update(config_updates)\n",
        "\n",
        "    config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config_utils.write_config(str(config_path), config)\n",
        "\n",
        "    for dir in output_subdir:\n",
        "        (output_dir / dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def prepare_environment():\n",
        "    cprint(\"Preparing environment...\", color=\"green\")\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF']   = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]      = \"3\"\n",
        "    os.environ[\"PYTHONWARNINGS\"]            = \"ignore\"\n",
        "\n",
        "def custom_download(custom_dirs):\n",
        "    filtered_urls = filter_dict_items(default_model_urls)\n",
        "\n",
        "    for key, value in custom_dirs.items():\n",
        "        urls = value.url.split(\",\")\n",
        "        dst = value.dst\n",
        "\n",
        "        if key == \"model\":\n",
        "            urls.extend(filtered_urls)\n",
        "\n",
        "        if urls[0]:\n",
        "            print_line(80, color=\"green\")\n",
        "            cprint(f\" [-] Downloading Custom {key}...\", color=\"flat_yellow\")\n",
        "\n",
        "        for url in urls:\n",
        "            url = url.strip()\n",
        "            if url != \"\":\n",
        "                print_line(80, color=\"green\")\n",
        "                if \"|\" in url:\n",
        "                    url, filename = map(str.strip, url.split(\"|\"))\n",
        "                    if not filename.endswith((\".safetensors\", \".ckpt\", \".pt\", \"pth\")):\n",
        "                        filename = filename + Path(get_filename(url)).suffix\n",
        "                else:\n",
        "                    filename = get_filename(url)\n",
        "\n",
        "                download(url=url, filename=filename, dst=dst, quiet=False)\n",
        "\n",
        "def filter_dict_items(dict_items):\n",
        "    result_list = []\n",
        "    for key, url in dict_items.items():\n",
        "        if globals().get(key):\n",
        "            result_list.append(url)\n",
        "    return result_list\n",
        "\n",
        "def auto_select_file(target_dir, config_key, file_types):\n",
        "    valid_files = [f for f in os.listdir(target_dir) if f.endswith(file_types)]\n",
        "    if valid_files:\n",
        "        file_path = random.choice(valid_files)\n",
        "\n",
        "        if Path(target_dir).joinpath(file_path).exists():\n",
        "            config = config_utils.read_config(str(config_file_path))\n",
        "            config[config_key] = file_path\n",
        "            config_utils.write_config(str(config_file_path), config)\n",
        "        return file_path\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def ui_config_presets():\n",
        "    preset_prompt = \"masterpiece, best quality, very aesthetic, absurdres\"\n",
        "    preset_negative_prompt = \"nsfw, lowres, (bad), text, error, fewer, extra, missing, worst quality, jpeg artifacts, low quality, watermark, unfinished, displeasing, oldest, early, chromatic aberration, signature, extra digits, artistic error, username, scan, [abstract]\"\n",
        "\n",
        "    return {\n",
        "        \"txt2img/Prompt/value\"              : preset_prompt,\n",
        "        \"txt2img/Negative prompt/value\"     : preset_negative_prompt,\n",
        "        \"img2img/Prompt/value\"              : preset_prompt,\n",
        "        \"img2img/Negative prompt/value\"     : preset_negative_prompt,\n",
        "        \"customscript/sampler.py/txt2img/Sampling method/value\" : \"Euler a\",\n",
        "        \"customscript/sampler.py/txt2img/Sampling steps/value\"  : 28,\n",
        "        \"customscript/sampler.py/txt2img/Scheduler/value\"       : \"Automatic\",\n",
        "    }\n",
        "\n",
        "def ui_config_settings(ui_config_file: str):\n",
        "    config = config_utils.read_config(str(ui_config_file))\n",
        "    preset_config = ui_config_presets()\n",
        "\n",
        "    for key, value in preset_config.items():\n",
        "        config[key] = value\n",
        "\n",
        "    config_utils.write_config(str(ui_config_file), config)\n",
        "\n",
        "def general_config_presets(config_file: str, lora_dir: str, use_presets: bool, ui_config_file: str):\n",
        "    config = config_utils.read_config(str(config_file))\n",
        "\n",
        "    config.update({\n",
        "        \"CLIP_stop_at_last_layers\"      : 2,\n",
        "        \"show_progress_every_n_steps\"   : 10,\n",
        "        \"show_progressbar\"              : True,\n",
        "        \"samples_filename_pattern\"      : \"[model_name]_[seed]\",\n",
        "        \"show_progress_type\"            : \"Approx NN\",\n",
        "        \"live_preview_content\"          : \"Prompt\",\n",
        "        \"forge_preset\"                  : \"xl\",\n",
        "        \"xl_t2i_width\"                  : 832,\n",
        "        \"xl_t2i_height\"                 : 1216,\n",
        "        \"xl_t2i_cfg\"                    : 7,\n",
        "        \"xl_t2i_hr_cfg\"                 : 7,\n",
        "        \"xl_t2i_sampler\"                : \"Euler a\",\n",
        "        \"xl_t2i_scheduler\"              : \"Automatic\",\n",
        "        \"gradio_theme\"                  : gradio_theme,\n",
        "    })\n",
        "\n",
        "    config_utils.write_config(str(config_file), config)\n",
        "\n",
        "    if use_presets:\n",
        "        ui_config_settings(ui_config_file)\n",
        "\n",
        "def is_valid(target_dir, file_types):\n",
        "    return any(f.endswith(file_types) for f in os.listdir(target_dir))\n",
        "\n",
        "def parse_args(config):\n",
        "    args = []\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args.append(f'\"{v}\"')\n",
        "        elif isinstance(v, str):\n",
        "            args.append(f'--{k}=\"{v}\"')\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args.append(f\"--{k}\")\n",
        "        elif isinstance(v, (float, int)) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "    return \" \".join(args)\n",
        "\n",
        "def main():\n",
        "    global output_dir, auto_select_model, auto_select_vae\n",
        "\n",
        "    ################################\n",
        "    # MAIN EXECUTION\n",
        "    ################################\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    start_time = time.time()\n",
        "    output_dir = mount_drive_function(drive_dir)\n",
        "\n",
        "    gpu_info    = py_utils.get_gpu_info(get_gpu_name=True)\n",
        "    python_info = py_utils.get_python_version()\n",
        "    torch_info  = py_utils.get_torch_version()\n",
        "\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\" [-] Current GPU: {gpu_info}\", color=\"flat_yellow\")\n",
        "    cprint(f\" [-] Python {python_info}\", color=\"flat_yellow\")\n",
        "    cprint(f\" [-] Torch {torch_info}\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    try:\n",
        "        install_dependencies()\n",
        "\n",
        "        print_line(80, color=\"green\")\n",
        "        install_webui(repo_dir, cprint(\"Unpacking Web UI Forge\", color=\"green\", tqdm_desc=True))\n",
        "        prepare_environment()\n",
        "\n",
        "        configure_output_path(config_file_path, output_dir, output_subdir)\n",
        "\n",
        "        print_line(80, color=\"green\")\n",
        "        if update_webui and not commit_hash:\n",
        "            update_repo(cwd=repo_dir, args=\"-X theirs --rebase --autostash\")\n",
        "        elif commit_hash:\n",
        "            reset_repo(repo_dir, commit_hash)\n",
        "\n",
        "        setup_directories()\n",
        "\n",
        "        repo_name, current_commit_hash, current_branch = validate_repo(repo_dir)\n",
        "        cprint(f\"Using '{repo_name}' repository...\", color=\"green\")\n",
        "        cprint(f\"Branch: {current_branch}, Commit hash: {current_commit_hash}\", color=\"green\")\n",
        "\n",
        "        if update_extensions:\n",
        "            print_line(80, color=\"green\")\n",
        "            batch_update(fetch=True, directory=extensions_dir, desc=cprint(\"Updating extensions\", color=\"green\", tqdm_desc=True))\n",
        "\n",
        "        elapsed_time = py_utils.calculate_elapsed_time(start_time)\n",
        "        print_line(80, color=\"green\")\n",
        "        cprint(f\"Finished installation. Took {elapsed_time}.\", color=\"flat_yellow\")\n",
        "    except Exception as e:\n",
        "        cprint(f\"An error occurred: {str(e)}\", color=\"red\")\n",
        "        print_line(80, color=\"red\")\n",
        "        cprint(\"Setup failed. Please check the error message above and try again.\", color=\"red\")\n",
        "        print_line(80, color=\"red\")\n",
        "        return\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    custom_download(custom_dirs)\n",
        "\n",
        "    elapsed_time = py_utils.calculate_elapsed_time(start_time)\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\"Download finished. Took {elapsed_time}.\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "    cprint(f\"Launching '{repo_name}'\", color=\"flat_yellow\")\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    if not is_valid(ckpt_dir, ('.ckpt', '.safetensors')):\n",
        "        cprint(f\"No checkpoints were found in the directory '{ckpt_dir}'.\", color=\"yellow\")\n",
        "        url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.1/blob/main/animagine-xl-3.1.safetensors\"\n",
        "        filename = get_filename(url)\n",
        "        aria2_download(url=url, download_dir=ckpt_dir, filename=filename)\n",
        "        print_line(80, color=\"green\")\n",
        "        auto_select_model = True\n",
        "\n",
        "    if not is_valid(vae_dir, ('.vae.pt', '.vae.safetensors', '.pt', '.ckpt')):\n",
        "        cprint(f\"No VAEs were found in the directory '{vae_dir}'.\", color=\"yellow\")\n",
        "        url = \"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/sdxl.vae.safetensors\"\n",
        "        filename = get_filename(url)\n",
        "        aria2_download(url=url, download_dir=vae_dir, filename=filename)\n",
        "        print_line(80, color=\"green\")\n",
        "        auto_select_vae = True\n",
        "\n",
        "    if auto_select_model:\n",
        "        selected_model  = auto_select_file(ckpt_dir, \"sd_model_checkpoint\", ('.ckpt', '.safetensors'))\n",
        "        cprint(f\"Selected Model: {selected_model}\", color=\"green\")\n",
        "\n",
        "    if auto_select_vae:\n",
        "        selected_vae    = auto_select_file(vae_dir, \"sd_vae\", ('.vae.pt', '.vae.safetensors', '.pt', '.ckpt'))\n",
        "        cprint(f\"Selected VAE: {selected_vae}\", color=\"green\")\n",
        "\n",
        "    print_line(80, color=\"green\")\n",
        "\n",
        "    general_config_presets(config_file_path, lora_dir, use_presets, ui_config_file_path)\n",
        "\n",
        "    if use_gradio_auth:\n",
        "      cprint(\"Gradio Auth (use this account to login):\", color=\"green\")\n",
        "      cprint(\"[-] Username: cagliostro\", color=\"green\")\n",
        "      cprint(\"[-] Password:\", password, color=\"green\")\n",
        "      print_line(80, color=\"green\")\n",
        "\n",
        "    config = {\n",
        "        \"enable-insecure-extension-access\": True,\n",
        "        \"disable-safe-unpickle\"           : True,\n",
        "        \"share\"                           : True if not ngrok_token else False,\n",
        "        \"ngrok\"                           : ngrok_token if ngrok_token else None,\n",
        "        \"ngrok-region\"                    : ngrok_region if ngrok_token else None,\n",
        "        \"gradio-auth\"                     : f\"{user}:{password}\" if use_gradio_auth else None,\n",
        "        \"no-hashing\"                      : True,\n",
        "        \"disable-console-progressbars\"    : True,\n",
        "        \"lowram\"                          : True,\n",
        "        \"opt-sub-quad-attention\"          : True,\n",
        "        \"opt-channelslast\"                : True,\n",
        "        \"no-download-sd-model\"            : True,\n",
        "        \"gradio-queue\"                    : True,\n",
        "        \"listen\"                          : True,\n",
        "        \"ckpt-dir\"                        : ckpt_dir,\n",
        "        \"vae-dir\"                         : vae_dir,\n",
        "        \"lora-dir\"                        : lora_dir,\n",
        "    }\n",
        "\n",
        "    args = parse_args(config)\n",
        "    final_args = f\"python launch.py {args} {additional_arguments}\"\n",
        "\n",
        "    cprint()\n",
        "    os.chdir(repo_dir)\n",
        "    ! {final_args}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "YYzHDlgEkkrY",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6f0313-a453-4eea-b65e-ddd9df2e9089"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Current GPU: Tesla T4\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Torch 2.4.1+cu121\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mInstalling ubuntu dependencies\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0;32mUnpacking Web UI Forge: 100%|██████████| 2/2 [00:14<00:00,  7.09s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0;32mPreparing environment...\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32m'lllyasviel/stable-diffusion-webui-forge' updated to the latest version\u001b[0m\n",
            "\u001b[0m\u001b[0;32mUsing 'lllyasviel/stable-diffusion-webui-forge' repository...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mBranch: main, Commit hash: 0c97a5347a424741f40c66aabec6ee595aeb5895\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0;32mUpdating extensions: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[0;32m [-] 'hako-mikan/sd-webui-supermerger' updated to the latest version\u001b[0m\n",
            "\u001b[0m\u001b[0;32m [-] 'zanllp/sd-webui-infinite-image-browsing' updated to the latest version\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mFinished installation. Took 34 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mStarting download of 'animagine-xl-3.1.safetensors' with aria2c...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mDownload of 'animagine-xl-3.1.safetensors' completed. Took 39 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0m [-] Downloading Custom vae...\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mStarting download of 'sdxl.vae.safetensors' with aria2c...\u001b[0m\n",
            "\u001b[0m\u001b[0;32mDownload of 'sdxl.vae.safetensors' completed. Took 2 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mDownload finished. Took 42 sec.\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[38;2;255;204;0mLaunching 'lllyasviel/stable-diffusion-webui-forge'\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0;32mSelected VAE: sdxl.vae.safetensors\u001b[0m\n",
            "\u001b[0m\u001b[0;32m================================================================================\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\n",
            "Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
            "Version: f2.0.1v1.10.1-previous-583-g0c97a534\n",
            "Commit hash: 0c97a5347a424741f40c66aabec6ee595aeb5895\n",
            "Installing SD-Hub requirement: \u001b[38;5;39mpv\u001b[0m\n",
            "Legacy Preprocessor init warning: Unable to install insightface automatically. Please try run `pip install insightface` manually.\n",
            "Launching Web UI with arguments: --enable-insecure-extension-access --disable-safe-unpickle --share --no-hashing --disable-console-progressbars --lowram --opt-sub-quad-attention --opt-channelslast --no-download-sd-model --gradio-queue --listen --lowram --theme dark --no-half-vae --opt-sdp-attention\n",
            "Total VRAM 15102 MB, total RAM 12979 MB\n",
            "pytorch version: 2.4.1+cu121\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype preferences: [torch.float32] -> torch.float32\n",
            "CUDA Using Stream: False\n",
            "Using pytorch cross attention\n",
            "Using pytorch attention for VAE\n",
            "themes/theme_schema@0.0.1.json: 100% 13.2k/13.2k [00:00<00:00, 60.3MB/s]\n",
            "ControlNet preprocessor location: /content/stable-diffusion-webui-forge/models/ControlNetPreprocessor\n",
            "Tag Autocomplete: Could not locate model-keyword extension, Lora trigger word completion will be limited to those added through the extra networks menu.\n",
            "\u001b[38;5;208m▶\u001b[0m SD-Hub: \u001b[38;5;39mv4.9.1\u001b[0m\n",
            "[Vec. CC] Style Sheet Loaded...\n",
            "Loading additional modules ... done.\n",
            "2024-10-23 08:56:47,530 - ControlNet - \u001b[0;32mINFO\u001b[0m - ControlNet UI callback registered.\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://ce55a94d038b8644c5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "\u001b[92mIIB Database file has been successfully backed up to the backup folder.\u001b[0m\n",
            "Startup time: 55.0s (prepare environment: 15.4s, import torch: 20.2s, other imports: 0.8s, opts onchange: 0.3s, load scripts: 3.7s, initialize google blockly: 6.2s, create ui: 4.7s, gradio launch: 3.1s, app_started_callback: 0.4s).\n",
            "Environment vars changed: {'stream': False, 'inference_memory': 1024.0, 'pin_shared_memory': False}\n",
            "[GPU Setting] You will use 93.22% GPU memory (14078.00 MB) to load weights, and use 6.78% GPU memory (1024.00 MB) to do matrix computation.\n",
            "Loading Model: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "[Unload] Trying to free all memory for cuda:0 with 0 models keep loaded ... Done.\n",
            "StateDict Keys: {'unet': 1680, 'vae': 248, 'text_encoder': 196, 'text_encoder_2': 518, 'ignore': 0}\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "K-Model Created: {'storage_dtype': torch.float16, 'computation_dtype': torch.float16}\n",
            "Model loaded in 27.3s (unload existing model: 0.5s, forge model load: 26.8s).\n",
            "Downloading VAEApprox model to: /content/stable-diffusion-webui-forge/models/VAE-approx/vaeapprox-sdxl.pt\n",
            "100% 209k/209k [00:00<00:00, 10.4MB/s]\n",
            "[Unload] Trying to free 3051.58 MB for cuda:0 with 0 models keep loaded ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9979.66 MB, Model Require: 1559.68 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 7395.98 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 7.18 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 8180.13 MB ... Done.\n",
            "[Unload] Trying to free 2856.18 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8179.28 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 8179.28 MB, Model Require: 0.00 MB, Previously Loaded: 4897.05 MB, Inference Require: 1024.00 MB, Remaining: 7155.28 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.04 seconds\n",
            "100% 28/28 [00:23<00:00,  1.17it/s]\n",
            "[Unload] Trying to free 8820.57 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8170.78 MB ... Unload model JointTextEncoder Current free memory is 9934.97 MB ... Done.\n",
            "[Memory Management] Target: IntegratedAutoencoderKL, Free GPU: 9934.97 MB, Model Require: 319.11 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 8591.86 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.65 seconds\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': ['/content/stable-diffusion-webui-forge/models/VAE/sdxl.vae.safetensors'], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Loading Model: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': ['/content/stable-diffusion-webui-forge/models/VAE/sdxl.vae.safetensors'], 'unet_storage_dtype': None}\n",
            "[Unload] Trying to free all memory for cuda:0 with 0 models keep loaded ... Current free memory is 9617.53 MB ... Unload model KModel Current free memory is 14616.84 MB ... Unload model IntegratedAutoencoderKL Done.\n",
            "StateDict Keys: {'unet': 1680, 'vae': 250, 'text_encoder': 196, 'text_encoder_2': 518, 'ignore': 0}\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "IntegratedAutoencoderKL Unexpected: ['model_ema.decay', 'model_ema.num_updates']\n",
            "K-Model Created: {'storage_dtype': torch.float16, 'computation_dtype': torch.float16}\n",
            "Model loaded in 33.2s (unload existing model: 5.8s, forge model load: 27.4s).\n",
            "Skipping unconditional conditioning when CFG = 1. Negative Prompts are ignored.\n",
            "[Unload] Trying to free 3051.58 MB for cuda:0 with 0 models keep loaded ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9936.82 MB, Model Require: 1559.68 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 7353.14 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 7.00 seconds\n",
            "[Unload] Trying to free 2881.78 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8171.12 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 8171.12 MB, Model Require: 0.00 MB, Previously Loaded: 4897.05 MB, Inference Require: 1024.00 MB, Remaining: 7147.12 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.04 seconds\n",
            " 61% 17/28 [00:08<00:05,  2.09it/s]\n",
            "[Unload] Trying to free 8990.72 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8170.86 MB ... Unload model JointTextEncoder Current free memory is 9935.09 MB ... Done.\n",
            "[Memory Management] Target: IntegratedAutoencoderKL, Free GPU: 9935.09 MB, Model Require: 319.11 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 8591.98 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.65 seconds\n",
            "Skipping unconditional conditioning when CFG = 1. Negative Prompts are ignored.\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 9615.80 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9615.80 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6838.82 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.48 seconds\n",
            "[Unload] Trying to free 1290.24 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7851.34 MB ... Done.\n",
            "100% 28/28 [00:13<00:00,  2.13it/s]\n",
            "[Unload] Trying to free 8575.88 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7850.84 MB ... Unload model JointTextEncoder Current free memory is 9614.45 MB ... Done.\n",
            "Memory cleanup has taken 0.74 seconds\n",
            "Skipping unconditional conditioning when CFG = 1. Negative Prompts are ignored.\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 9614.70 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9614.70 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6837.72 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.49 seconds\n",
            "[Unload] Trying to free 1290.24 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7850.23 MB ... Done.\n",
            "100% 28/28 [00:12<00:00,  2.30it/s]\n",
            "[Unload] Trying to free 8575.88 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7851.70 MB ... Unload model JointTextEncoder Current free memory is 9615.31 MB ... Done.\n",
            "Memory cleanup has taken 0.74 seconds\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Loading Model: {'checkpoint_info': {'filename': '/content/stable-diffusion-webui-forge/models/Stable-diffusion/animagine-xl-3.1.safetensors', 'hash': 'c798d390'}, 'additional_modules': [], 'unet_storage_dtype': None}\n",
            "[Unload] Trying to free all memory for cuda:0 with 0 models keep loaded ... Current free memory is 9615.56 MB ... Unload model KModel Current free memory is 14615.58 MB ... Unload model IntegratedAutoencoderKL Done.\n",
            "StateDict Keys: {'unet': 1680, 'vae': 248, 'text_encoder': 196, 'text_encoder_2': 518, 'ignore': 0}\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "K-Model Created: {'storage_dtype': torch.float16, 'computation_dtype': torch.float16}\n",
            "Model loaded in 28.6s (unload existing model: 2.8s, forge model load: 25.8s).\n",
            "[Unload] Trying to free 3051.58 MB for cuda:0 with 0 models keep loaded ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9936.82 MB, Model Require: 1559.68 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 7353.14 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 7.09 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 8171.37 MB ... Done.\n",
            "[Unload] Trying to free 2615.54 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8170.68 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 8170.68 MB, Model Require: 0.00 MB, Previously Loaded: 4897.05 MB, Inference Require: 1024.00 MB, Remaining: 7146.68 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.05 seconds\n",
            "100% 28/28 [00:08<00:00,  3.30it/s]\n",
            "[Unload] Trying to free 3137.35 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8170.51 MB ... Done.\n",
            "[Memory Management] Target: IntegratedAutoencoderKL, Free GPU: 8170.51 MB, Model Require: 319.11 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6827.39 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.12 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7850.68 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7850.07 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7849.38 MB ... Done.\n",
            "100% 28/28 [00:08<00:00,  3.44it/s]\n",
            "[Unload] Trying to free 2722.50 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7849.22 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7849.30 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7848.61 MB ... Done.\n",
            "100% 28/28 [00:07<00:00,  3.60it/s]\n",
            "[Unload] Trying to free 2722.50 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7849.22 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7849.30 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7848.69 MB ... Done.\n",
            "[Unload] Trying to free 1264.64 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7847.83 MB ... Done.\n",
            "100% 28/28 [00:25<00:00,  1.11it/s]\n",
            "[Unload] Trying to free 8405.72 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7847.35 MB ... Unload model JointTextEncoder Current free memory is 9611.58 MB ... Done.\n",
            "Memory cleanup has taken 0.96 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 9611.82 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9611.82 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6834.84 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.75 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7845.73 MB ... Done.\n",
            "[Unload] Trying to free 1296.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7844.87 MB ... Done.\n",
            "100% 28/28 [00:27<00:00,  1.02it/s]\n",
            "[Unload] Trying to free 8614.16 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7846.08 MB ... Unload model JointTextEncoder Current free memory is 9611.56 MB ... Done.\n",
            "Memory cleanup has taken 0.80 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 9611.81 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9611.81 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6834.83 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.67 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7845.72 MB ... Done.\n",
            "[Unload] Trying to free 1612.80 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7844.79 MB ... Done.\n",
            "100% 28/28 [00:33<00:00,  1.20s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7844.18 MB ... Unload model JointTextEncoder Current free memory is 9609.66 MB ... Unload model KModel Current free memory is 14609.69 MB ... Done.\n",
            "Memory cleanup has taken 5.28 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14610.00 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14610.00 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11833.02 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.56 seconds\n",
            "[Unload] Trying to free 7978.96 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12845.27 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12845.27 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6924.22 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.57 seconds\n",
            "100% 28/28 [00:34<00:00,  1.22s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7845.22 MB ... Unload model JointTextEncoder Current free memory is 9609.03 MB ... Unload model KModel Current free memory is 14609.69 MB ... Done.\n",
            "Memory cleanup has taken 2.18 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14610.00 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14610.00 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11833.02 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.55 seconds\n",
            "[Unload] Trying to free 7978.96 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12845.27 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12845.27 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6924.22 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.52 seconds\n",
            "100% 28/28 [00:33<00:00,  1.20s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7843.99 MB ... Unload model JointTextEncoder Current free memory is 9607.80 MB ... Unload model KModel Current free memory is 14608.46 MB ... Done.\n",
            "Memory cleanup has taken 2.08 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14608.77 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14608.77 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11831.79 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.55 seconds\n",
            "[Unload] Trying to free 7978.96 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12844.04 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12844.04 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6922.99 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.50 seconds\n",
            "100% 28/28 [00:33<00:00,  1.21s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7843.99 MB ... Unload model JointTextEncoder Current free memory is 9607.80 MB ... Unload model KModel Current free memory is 14608.46 MB ... Done.\n",
            "Memory cleanup has taken 2.25 seconds\n",
            "[Unload] Trying to free 7978.96 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14608.46 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 14608.46 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 8687.41 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.45 seconds\n",
            "100% 28/28 [00:33<00:00,  1.18s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 9608.29 MB ... Unload model KModel Current free memory is 14607.84 MB ... Done.\n",
            "Memory cleanup has taken 1.57 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14608.15 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14608.15 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11831.17 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.53 seconds\n",
            "[Unload] Trying to free 7978.96 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12843.42 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12843.42 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6922.37 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.38 seconds\n",
            "100% 28/28 [00:33<00:00,  1.19s/it]\n",
            "[Unload] Trying to free 10719.84 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7842.15 MB ... Unload model JointTextEncoder Current free memory is 9605.96 MB ... Unload model KModel Current free memory is 14606.62 MB ... Done.\n",
            "Memory cleanup has taken 1.99 seconds\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14608.15 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14608.15 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11831.18 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.51 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 12843.73 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 12843.73 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 12843.12 MB ... Done.\n",
            "[Unload] Trying to free 8696.95 MB for cuda:0 with 1 models keep loaded ... Current free memory is 12798.21 MB ... Done.\n",
            "[Unload] Trying to free 12817.36 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12824.93 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12824.93 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6903.89 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.61 seconds\n",
            "100% 28/28 [03:07<00:00,  6.70s/it]\n",
            "[Unload] Trying to free 42879.38 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7819.17 MB ... Unload model JointTextEncoder Current free memory is 9582.98 MB ... Unload model KModel Current free memory is 14582.99 MB ... Done.\n",
            "Memory cleanup has taken 2.08 seconds\n",
            "[Unload] Trying to free 7068.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 14565.32 MB ... Done.\n",
            "[Unload] Trying to free 3302.87 MB for cuda:0 with 0 models keep loaded ... Current free memory is 14601.57 MB ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 14601.57 MB, Model Require: 1752.98 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 11824.59 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.58 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 12837.13 MB ... Done.\n",
            "[Unload] Trying to free 7676.88 MB for cuda:0 with 0 models keep loaded ... Current free memory is 12836.88 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 12836.88 MB, Model Require: 4897.05 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6915.83 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 1.51 seconds\n",
            "100% 16/16 [00:14<00:00,  1.09it/s]\n",
            "[Unload] Trying to free 8712.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7825.00 MB ... Unload model JointTextEncoder Current free memory is 9589.43 MB ... Done.\n",
            "Memory cleanup has taken 0.54 seconds\n",
            "Interrupted with signal 2 in <frame at 0x588d64eed020, file '/content/stable-diffusion-webui-forge/modules_forge/main_thread.py', line 43, code loop>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## **Download Generated Images**\n",
        "# @markdown Download file manually from files tab or save to Google Drive\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from colablib.colored_print import cprint\n",
        "\n",
        "os.chdir(output_dir)\n",
        "\n",
        "use_drive = True  # @param {type:\"boolean\"}\n",
        "folder_name = \"cagliostro-forge-colab\"  # @param {type: \"string\"}\n",
        "filename = \"waifu.zip\"  # @param {type: \"string\"}\n",
        "save_as = filename\n",
        "\n",
        "def get_unique_filename(base_filename):\n",
        "    path = Path(base_filename)\n",
        "    if not path.exists():\n",
        "        return path\n",
        "    i = 1\n",
        "    while True:\n",
        "        new_path = path.with_name(f\"{path.stem}({i}){path.suffix}\")\n",
        "        if not new_path.exists():\n",
        "            return new_path\n",
        "        i += 1\n",
        "\n",
        "filename = get_unique_filename(filename)\n",
        "\n",
        "def zip_directory(directory, zipname):\n",
        "    with zipfile.ZipFile(zipname, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_path in directory.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                zipf.write(file_path, file_path.relative_to(directory.parent))\n",
        "\n",
        "zip_directory(output_dir, Path('/content/outputs.zip'))\n",
        "\n",
        "if use_drive:\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive_service = GoogleDrive(gauth)\n",
        "\n",
        "    def create_folder(folder_name):\n",
        "        query = f\"title='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "        file_list = drive_service.ListFile({\"q\": query}).GetList()\n",
        "        if file_list:\n",
        "            cprint(\"Debug: Folder exists\", color=\"green\")\n",
        "            return file_list[0][\"id\"]\n",
        "        else:\n",
        "            cprint(\"Debug: Creating folder\", color=\"green\")\n",
        "            folder = drive_service.CreateFile({\n",
        "                \"title\": folder_name,\n",
        "                \"mimeType\": \"application/vnd.google-apps.folder\"\n",
        "            })\n",
        "            folder.Upload()\n",
        "            return folder[\"id\"]\n",
        "\n",
        "    def upload_file(file_path, folder_id, save_as):\n",
        "        save_as = get_unique_filename(save_as)\n",
        "        file = drive_service.CreateFile({\"title\": save_as.name, \"parents\": [{\"id\": folder_id}]})\n",
        "        file.SetContentFile(str(file_path))\n",
        "        file.Upload()\n",
        "        file.InsertPermission({\"type\": \"anyone\", \"value\": \"anyone\", \"role\": \"reader\"})\n",
        "        return file[\"id\"]\n",
        "\n",
        "    folder_id = create_folder(folder_name)\n",
        "    file_id = upload_file(Path('/content/outputs.zip'), folder_id, Path(save_as))\n",
        "    sharing_link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "    cprint(f\"Your sharing link: {sharing_link}\", color=\"green\")\n",
        "else:\n",
        "    cprint(\"Files zipped locally. Download manually from the files tab.\", color=\"yellow\")\n"
      ],
      "metadata": {
        "id": "UyTKsCa1qUL4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4758c064-0cd3-4722-cb73-c274394d8bf3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[0;33mFiles zipped locally. Download manually from the files tab.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}